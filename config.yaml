# ============================================================================
# scARKIDS Configuration File - Session 1 (Supervised Initialization)
# ============================================================================
# This configuration is optimized for:
# - Supervised learning (cell types known)
# - First training session (model initialization)
# - ~1 hour training time on GPU (batch_size=256, n_epochs=40)
# - Well-curated dataset with high-quality labels
# ============================================================================


# ============================================================================
# Global Parameters (Shared Across Modules)
# ============================================================================
global:
  # Core dimensions (UNTUNABLE - architecture-critical)
  latent_dim: 20                    # Latent space dimension (moderate)
  n_diffusion_steps: 400            # Diffusion timesteps (standard)
  
  # Data dimensions (determined by dataset)
  n_genes: 2000                     # Anchor genes (adjust to your dataset)
  n_batches: 8                      # Number of batches (adjust to your dataset)
  n_cell_types: 20                  # Cell types (adjust to your dataset)
  
  # Model mode
  supervised: true                  # Session 1: SUPERVISED (cell types known)
  
  # Numerical stability
  eps: 1.0e-8                       # Small constant (keep fixed)
  
  # Regularization
  dropout: 0.1                      # Light dropout for initialization
  use_batch_norm: true              # Batch norm (better than layer norm)


# ============================================================================
# Data Configuration
# ============================================================================
data:
  # Dataset paths (adjust to your data locations)
  train_data_path: "./data/session3_sup_aortic/HumanAorticData_train.h5ad"
  val_data_path: "./data/session3_sup_aortic/HumanAorticData_val.h5ad"
  test_data_path: "./data/session3_sup_aortic/HumanAorticData_test.h5ad"
  
  # Data preprocessing
  log_transform: true               # Log1p transformation (standard)
  normalize: true                   # Normalize by library size
  
  # Cell type and batch information (adjust keys to match your H5AD)
  cell_type_key: "cell_type"         # Key in adata.obs for cell type labels
  batch_key: "batch"                # Key in adata.obs for batch labels
  
  # Optional: Empirical priors (null = compute from data)
  cell_type_prior_probs: null       # Compute from training data
  batch_prior_probs: null           # Compute from training data


# ============================================================================
# Module Configurations
# ============================================================================

# Encoder Module (VAE Encoder q_φ)
encoder:
  # Architecture (UNTUNABLE - checkpoint-critical)
  hidden_dims: [256, 128]           # 2 layers, decreasing dims (fast convergence)
  use_layer_norm: false             # Batch norm preferred
  
  # Input preprocessing
  input_transform: "log1p"          # Log1p input preprocessing
  
  # Regularization
  dropout: 0.1                      # Inherited from global (light)
  use_batch_norm: true              # Inherited from global


# Classifier Module (Cell Type Classifier q_ω)
classifier:
  # Architecture (UNTUNABLE)
  hidden_dim: 256                   # Single-sized hidden layers
  n_layers: 3                       # Moderate depth
  batch_embed_dim: 32               # Batch embedding dimension
  
  # Regularization (slightly higher than encoder)
  dropout: 0.2                      # Extra regularization for classifier


# Likelihood Module (ZINB Decoder p_θ)
likelihood:
  # Decoder architecture (UNTUNABLE)
  hidden_dim: 128                   # Moderate size
  n_layers: 3                       # Match encoder depth
  
  # ZINB parameters
  dispersion_mode: "gene"           # Gene-specific dispersion
  use_library_size: true            # Include library size


# Prior Module (p(z^(0)|c), p(c), p(z^(T)))
prior:
  # Cell type prior probabilities
  cell_type_prior_probs: null       # Compute empirically from data
  
  # Batch prior probabilities
  batch_prior_probs: null           # Compute empirically from data


# DDPM Forward Process Module (diffusion noise schedule)
ddpm_forward:
  # Variance schedule
  beta_schedule: "cosine"           # Cosine schedule (smoother than linear)
  beta_min: 1.0e-4                  # Minimum β_t (standard)
  beta_max: 2.0e-2                  # Maximum β_t (standard)


# DDPM Backward Process Module (reverse denoising p_ψ)
ddpm_backward:
  # Reverse process variance (UNTUNABLE)
  variance_type: "learned"            # Fixed variance (stable for initialization)
  
  # Noise prediction network (UNTUNABLE)
  noise_hidden_dim: 128             # Moderate hidden dimension
  noise_n_layers: 2                 # Shallow network (fast convergence)
  
  # Embedding dimensions (UNTUNABLE)
  timestep_embed_dim: 64            # Timestep embedding
  celltype_embed_dim: 32            # Cell type embedding
  
  # Regularization
  dropout: 0.1                      # Inherited from global


# Variational Posterior Module (q_φ,ω)
# Note: Configuration derived from encoder, classifier, and ddpm_forward


# ELBO Module (Loss Computation)
# Note: Configuration derived from all atomic modules


# ============================================================================
# Training Configuration
# ============================================================================
training:
  # Training loop (optimized for ~1 hour)
  n_epochs: 30                      # 40 epochs ≈ 45-60 min on V100/A100
  batch_size: 256                   # Large batch: stable gradients, ~1000 batches/epoch
  
  # Optimization
  learning_rate: 1.0e-3             # AdamW default (proven robust)
  weight_decay: 1.0e-5              # Light L2 regularization
  grad_clip_norm: 0.5               # Gradient clipping (standard)
  
  # Learning rate scheduling
  lr_scheduler: "step"              # Step decay (better for short training)
                                    # Drops LR at epochs 13, 26, 39
  lr_warmup_epochs: 4               # 6/50 = 12% warmup (standard range)
  
  # Logging and checkpointing
  log_interval: 10                  # Log every 10 batches
  checkpoint_interval: 20           # Save checkpoint every 10 epochs (4 total)
  checkpoint_dir: "./checkpoints"   # Directory for checkpoints
  
  # Hardware
  device: "cuda"                    # GPU required for 1-hour target
  use_amp: false                    # Disable AMP (stability > speed)
  
  # Reproducibility
  seed: 42


# ============================================================================
# Session Configuration
# ===========================================================================
  sessions:
    # # Session 1: First supervised dataset (model initialization)
    # - name: "supervised_session_1_pancreas"
    #   data_path: "./data/session1_sup_pan/PancreasData_train.h5ad" #train split path
    #   supervised: true              # Ground-truth cell types (cell_type, batch)
    #   n_epochs: 100
    #   resume_from: null             # Initialize new model
    #   output_checkpoint: "./checkpoints/session_1_best.pt"
    
    # Session 2: First unsupervised dataset (fine-tuning)
    # - name: "unsupervised_session_2_lungs"
    #   data_path: "./data/session2_unsup_lungs/LungsData_Train.h5ad"
    #   supervised: false             # Infer cell types (batch, RUBBISH)
    #   n_epochs: 100                  # Shorter (fine-tuning)
    #   resume_from: null  # Load Session 1 best
    #   output_checkpoint: "./checkpoints/session_2_best.pt"
    
    # # Session 3: Second supervised dataset (supervised fine-tuning)
    - name: "supervised_session_3_aortic"
      data_path: "./data/session3_sup_aortic/HumanAorticData_train.h5ad"
      supervised: true              # Ground-truth cell types again (cell_type, batch)
      n_epochs: 30                  # Shorter (fine-tuning)
      resume_from: "./checkpoints/session_1_best.pt"  # Load Session 2 best
      output_checkpoint: "./checkpoints/session_3_best.pt"
    
    # # Session 4: Second unsupervised dataset (final refinement)
    # - name: "unsupervised_session_4"
    #   data_path: "./data/session4_unsup_new/ImmuneCellData_Train.h5ad"
    #   supervised: false             # Infer cell types (batch, RUBBISH)
    #   n_epochs: 50                  # Shorter (fine-tuning)
    #   resume_from: "./checkpoints/session_2_best.pt"  # Load Session 3 best
    #   output_checkpoint: "./checkpoints/session_4_best.pt"


# ============================================================================
# Inference Configuration
# ============================================================================
inference:
  # DDPM refinement
  use_ddpm_refinement: true         # Apply diffusion refinement
  n_refinement_steps: 100           # ~25% of T=400 (balances speed/quality)
  
  # Batch correction
  default_target_batch: 0           # Reference batch (adjust as needed)
  
  # Output options
  save_latent_embeddings: true      # Save z^(0) embeddings
  save_corrected_expression: true   # Save batch-corrected expression
  save_cell_type_predictions: true  # Save cell type predictions
  
  # Output path
  output_dir: "./results"


# ============================================================================
# Notes
# ============================================================================
# 
# 1. UNTUNABLE PARAMETERS (Architecture-Critical)
#    These define the model structure and CANNOT change between sessions:
#    - global.latent_dim
#    - global.n_diffusion_steps
#    - encoder.hidden_dims
#    - classifier.hidden_dim, n_layers, batch_embed_dim
#    - likelihood.hidden_dim, n_layers
#    - ddpm_backward.variance_type, noise_hidden_dim, noise_n_layers,
#      timestep_embed_dim, celltype_embed_dim
#    - global.use_batch_norm
#
# 2. TUNABLE PARAMETERS (Can adjust between sessions)
#    - n_epochs
#    - batch_size
#    - learning_rate
#    - lr_scheduler, lr_warmup_epochs
#    - dropout rates
#    - weight_decay
#    - ddpm_forward.beta_schedule parameters
#
# 3. SESSION-SPECIFIC PARAMETERS (Must update per session)
#    - sessions[].name
#    - sessions[].data_path
#    - sessions[].supervised
#    - sessions[].n_epochs (can change)
#    - sessions[].resume_from (path to previous checkpoint)
#    - sessions[].output_checkpoint
#
# 4. EXPECTED TIMING
#    - Session 1: 40 epochs × ~1.5 min/epoch ≈ 60 min (initialization)
#    - Sessions 2-4: 30 epochs × ~1.5 min/epoch ≈ 45 min (fine-tuning)
#
# 5. VALIDATION & MONITORING
#    - Monitor reconstruction loss (NLL) and KL terms
#    - Check gradient norms and learning rate schedule
#    - After Session 1, review logs and provide to AI for Session 2 recommendations
#
